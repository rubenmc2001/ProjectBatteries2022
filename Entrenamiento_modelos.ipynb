{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='indice'></a>\n",
    "\n",
    "## Índice\n",
    "### [Librerias](#librerias)\n",
    "### [Carga de datos](#carga)\n",
    "### [Pre-proceso](#pre)\n",
    "   1. ### [Variables cualitativas](#cuali)\n",
    "   2. ### [Valores nulos](#null)\n",
    "   3. ### [Variable Y (ESTADO_BAT)](#y)\n",
    "   4. ### [Conjunto train-test](#train)\n",
    "   5. ### [SMOTE](#smote)\n",
    "   6. ### [Optimizar los parámetros](#optimizar)\n",
    "   7. ### [Seleccionar la métrica y los diferentes modelos que se quieren probar](#metrica_modelos)\n",
    "   8. ### [Métrica Brier score multiclase](#brier_score)\n",
    "\n",
    "\n",
    "### [Modelos](#modelos)\n",
    "   - ### [Árboles de Clasificación](#arbol)\n",
    "   - ### [Regresión Logística](#regresion)\n",
    "   - ### [K-vecinos más cercanos](#KNN)\n",
    "   - ### [Máquinas de vectores soporte](#SVC)\n",
    "   - ### [Redes neuronales](#NN)\n",
    "   - ### [Bagging](#Bagging)\n",
    "   - ### [Boosting](#Boosting)\n",
    "   - ### [Stacking](#Stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias <a id='librerias'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las librerias utilizadas han sido las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import tree\n",
    "\n",
    "# Preprocesado\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Optimización de parámetros\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Modelado\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Configuración warnings\n",
    "# ------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos  <a id='carga'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos que utilizaremos para realizar el proyecto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('dataset_escalado.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-proceso <a id='pre'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Eliminamos las variables cualitativas. <a id='cuali'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ID_PDA','SN_PDA','FECHA_FAB','FECHA_TEST','SN_BAT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Esta función me permite eliminar de la base de datos los valores que son NaN y los Inf, en caso de que los haya. <a id='null'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Definimos el conjunto de variables X, junto con la variable objetivo Y (ESTADO_BAT). <a id='y'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['ESTADO_BAT'], axis=1).values\n",
    "y = df['ESTADO_BAT'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un histograma para observar la distribución de las clases presentes en la variable objetivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot('ESTADO_BAT',data=df,kind=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. División de los datos en un conjunto de train y test. <a id='train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df.drop(columns = 'ESTADO_BAT'),\n",
    "                                        df['ESTADO_BAT'],\n",
    "                                        random_state = 123,\n",
    "                                        stratify = df['ESTADO_BAT'],\n",
    "                                        train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Aplicamos el algoritmo SMOTE a los datos de train, para solucionar el problema del desbalanceo de clases. <a id='smote'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy='auto', k_neighbors=2, random_state=100)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Optimizamos los parámetros. A continuación, definimos una función para elegir los parámetros óptimos de los diferentes modelos probados. <a id='optimizar'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clfGrid(X, y, clasificadores, n_clas):\n",
    "    res = {}\n",
    "    for clf_cnt, clf in enumerate(clasificadores):\n",
    "        clf.fit(X, y)     \n",
    "        res[n_clas[clf_cnt]] = {}\n",
    "        res[n_clas[clf_cnt]][\"F1\"] = clf.best_score_\n",
    "        res[n_clas[clf_cnt]][\"Mejor clasificador\"] = clf.best_estimator_\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Seleccionar la métrica con la que maximizar los parámetros (en este caso f1_weighted) y los diferentes modelos que se quieren probar.  <a id='metrica_modelos'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada caso se elige el modelo y los valores o rango de valores que queremos probar en cada parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = make_scorer(f1_score , average='weighted')\n",
    "\n",
    "clasificadores = [GridSearchCV(estimator=LogisticRegression(), \n",
    "                               param_grid = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "                                             'solver' : ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'), \n",
    "                                             'max_iter' : [100, 200, 300]}, scoring=f1),\n",
    "                  \n",
    "                  GridSearchCV(estimator=AdaBoostClassifier(), \n",
    "                               param_grid = {'n_estimators':[50,100,200,500]}, \n",
    "                               scoring=f1),\n",
    "                  \n",
    "                  GridSearchCV(estimator=LinearSVC(), \n",
    "                               param_grid = {'C':[0.1,1,1.5]}, scoring=f1),\n",
    "                  \n",
    "                  GridSearchCV(estimator=KNeighborsClassifier(), \n",
    "                               param_grid = {'n_neighbors':[1,2,3,4,5,6], \n",
    "                                             'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                                             'p':[1,2]}, scoring=f1),\n",
    "                  \n",
    "                  GridSearchCV(estimator=DecisionTreeClassifier(), \n",
    "                               param_grid = {'criterion':['gini','entropy'],\n",
    "                                             'splitter':['random','best'],\n",
    "                                             'min_samples_split':[2,4,6], \n",
    "                                             'max_depth': [2, 5, 10, 15, 20], \n",
    "                                             'min_samples_leaf': [5, 4, 3, 2, 1]}, \n",
    "                               scoring=f1),\n",
    "                  \n",
    "                 GridSearchCV(estimator = MLPClassifier(random_state = 1), \n",
    "                              param_grid={'activation': ('identity', 'logistic', 'tanh', 'relu'), \n",
    "                                          'solver': ('lbfgs', 'sgd', 'adam'), 'max_iter': [2, 5, 10]}, \n",
    "                              scoring=f1),\n",
    "                  \n",
    "                 GridSearchCV(estimator = SVC(random_state = 1), \n",
    "                              param_grid = dict(C = [1,2,4], gamma = ['scale', 'auto'], \n",
    "                                                kernel = ['poly', 'rbf', 'sigmoid']), \n",
    "                              scoring=f1)]\n",
    "\n",
    "n_clas = ['Regressión logística', 'AdaBoost', 'SVC Lineal','Vecinos','Arboles', 'Perceptrón Multicapa', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGrid(X_train,y_train, clasificadores, n_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los parámetros óptimos de los diferentes clasificadores, pasaremos a aplicar cada modelo con sus respectivos parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo de la métrica f1_score:\n",
    "\n",
    "El F1-score, combina las medidas de precisón y recall (exhaustividad) lo que nos hace medir el rendimiento de un modelo de forma más fiable.\n",
    "\n",
    "Este valor oscila entre los valores 0 y 1. Siendo 1 el mejor valor pues indicaría que el modelo predice con 100% de fiabilidad. Por otro lado, el valor 0 indicaría que el modelo no se asemeja en nada a la realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precisión:**\n",
    "\n",
    "El número de valores de la clase positiva bien predichos dividido por el número de valores de la clase positiva bien predichos más el número de valores de la clase positiva mal predichos.\n",
    "\n",
    "$precision=\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n",
    "\n",
    "Indicando la calidad del modelo, pues muestra el acierto en la clasificación de los valores de la clase positiva.\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "El número de valores de la clase positiva bien predichos dividido por el número de valores de la clase positiva bien predichos más el número de valores de la clase negativa mal predichos.\n",
    "\n",
    "$recall=\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n",
    "\n",
    "Indicando la cantidad de que el modelo es capaz de predecir bien, pues muestra la proporción de valores clasificados como clase positiva frente a la realidad.\n",
    "\n",
    "**Fórmula F1_score:**\n",
    "\n",
    "$F_{1}=\\frac{\\text{Precisión} \\times \\text{Exhaustividad} \\times \\text{2}}{\\text{Precisión} + \\text{Exhaustividad}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. En caso de desbalanceo en las clases en la variable respuesta, utilizamos la métrica Brier score multiclase.  <a id='brier_score'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_multi(targets, probs):\n",
    "    return np.mean(np.sum((probs - targets)**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo de la métrica Brier Score:\n",
    "\n",
    "El Brier Score es una métrica que calcula el error de predicción basándose en la fiabilidad del clasificador.\n",
    "\n",
    "Sus valores oscilan entre 0 y 1. El 0 indica que no ha habido error, por tanto es el mejor caso. Por el contrario, si se da un número alto como el 1 indicaría que se ha fallado en todas las predicciones.\n",
    "\n",
    "Debido a su cálculo en las  predicciones unidimensionales, es estrictamente equivalente al error cuadrático medio aplicado a las probabilidades predichas.\n",
    "\n",
    "**Fórmula Brier Score:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N} \\sum_{t=1} ^{N} {(f_t - o_t)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos <a id='modelos'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo de Árboles de Clasificación  <a id='arbol'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo = DecisionTreeClassifier(max_depth = 10, splitter=\"random\",random_state=10)\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El Brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 de predict da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 cross-val da(Decision Tree): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo de Regresión Logística <a id='regresion'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo =  LogisticRegression(C=1, solver='liblinear')\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (Regresión Logística): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo de K-Vecinos más cercanos <a id='KNN'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo =  KNeighborsClassifier(n_neighbors=1, p=1)\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (KNN): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo de Máquinas de Vectores Soporte <a id='SVC'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo =  SVC(C=4, gamma='auto', random_state=1,probability=True)\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (SVC): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redes neuronales <a id='NN'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo =  MLPClassifier(random_state=1, max_iter=1000)\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (NN): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probamos Bagging con los diferentes clasificadores  <a id='Bagging'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=1, p=1),n_estimators=200)\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (Bagging): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probamos Boosting con los diferentes clasificadores <a id='Boosting'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "modelo=AdaBoostClassifier(base_estimator=LogisticRegression(C=0.1, max_iter=100, penalty='l2',\n",
    "                            random_state=10, solver='lbfgs'),n_estimators=200, algorithm='SAMME')\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (Boosting): {f1:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probamos Stacking con los diferentes clasificadores <a id='Stacking'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos los dos clasificadores con los que crearemos el modelo de Stacking\n",
    "estimators=[('mlp',MLPClassifier(random_state=1, max_iter=1000)),('LinearSVC', LinearSVC(C=0.1, max_iter=10000))]\n",
    "# Creamos el modelo\n",
    "modelo=StackingClassifier(estimators=estimators,final_estimator=KNeighborsClassifier(n_neighbors=1, p=1))\n",
    "# Validación cruzada para ver los resultados del modelo\n",
    "f1 = cross_val_score(modelo, X_train,y_train,cv=5 ,scoring = 'f1_weighted').mean()\n",
    "# Entrenamos el modelo\n",
    "fit = modelo.fit(X_train,y_train)\n",
    "# Predecimos los datos de test\n",
    "pred = modelo.predict(X_test)\n",
    "# Calculamos la probabilidad que el modelo asigna a cada observación de pertenecer a cada clase\n",
    "probs = modelo.predict_proba(X_test)\n",
    "\n",
    "bien = []\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        bien.append([1,0,0,0])\n",
    "    elif i == 25:\n",
    "        bien.append([0,1,0,0])\n",
    "    elif i == 50:\n",
    "        bien.append([0,0,1,0])\n",
    "    elif i == 75:\n",
    "        bien.append([0,0,0,1])\n",
    "bien = np.array(bien)\n",
    "print('Los valores de las métricas para dicho modelo son los siguientes: ')\n",
    "print(f'El brier Score da:',brier_multi(probs,bien))\n",
    "print(f'El f1 predicho da:',f1_score(y_test,pred, average='weighted'))\n",
    "print(f'F1 (Stacking): {f1:.5f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PL3_S1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
